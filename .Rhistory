colnames(word.table) <- c("line", "Freq", "ids")
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
#full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_66')
library(rJava)
library(xlsx)
library(stringr)
library(twitteR)
library(jsonlite)
line <- NULL
current.tweet <- NULL
tweet.table <- data.frame(stringsAsFactors = FALSE)
user.table <-NULL
#---2ND PART - TWEET ANALYSIS ---------------------------------------------------------------------
#--Identifying all the CVS files in the Home directory and extrating the data from each -----------
get.files <- list.files()
tweets.location <- str_detect(get.files, "tweet.csv")
users.location <- str_detect(get.files, "user.csv")
tweet.files <- get.files[tweets.location]
user.files <- get.files[users.location]
tweet.table <-  read.csv(tweet.files[1])
for(j in 2:length(tweet.files)){tweet.table <-  rbind(tweet.table, read.csv(tweet.files[j]))}
tweet.dups <- duplicated(tweet.table[9])
tweet.dups <- tweet.dups==FALSE
tweet.table <- tweet.table[tweet.dups,]
user.table <-  read.csv(user.files[1])
for(j in 2:length(user.files)){user.table <-  rbind(user.table, read.csv(user.files[j]))}
user.dups <- duplicated(user.table[15])
user.dups <- user.dups==FALSE
user.table <- user.table[user.dups,]
tweet.table <- merge(x=tweet.table, y=user.table, by.x="screenName", by.y ="screenName")
#---Converting the 'search.terms' values to user-friendly values ----------------------------------
tweet.table$search <- as.character(tweet.table$search)
tweet.table$search[tweet.table$search=="hancockbank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="hancock bank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="whitneybank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="whitney bank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="harrisonfinance"]<-"Harrison Finance"
tweet.table$search[tweet.table$search=="harrison finance"]<-"Harrison Finance"
tweet.text <- tweet.table$text
tweet.ids <- tweet.table$id.x
tweet.text <- lapply(tweet.text, toupper)
tweet.text <- str_replace_all( tweet.text,"[[:punct:]]", "")
tweet.text <- str_replace_all(tweet.text,"\n", "")
#---Extracting each valid word from each tweet ----------------------------------------------------
word.table <-NULL
for(i in 1:length(tweet.text)){
newline <- strsplit(tweet.text[i], split=" ")
current.tweet <- as.data.frame(table(newline))
current.tweet <- cbind(current.tweet, rep(tweet.ids[i],nrow(current.tweet)))
word.table <- rbind(word.table, current.tweet)
}
colnames(word.table) <- c("line", "Freq", "ids")
#---Compiling and removing any undesired word from the list to streamline the analysis ------------
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
View(tweet.table)
View(user.table)
user.table$followersCount
hist(user.table$followersCount)
hist(user.table$followersCount, breaks=100)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_66')
library(rJava)
library(xlsx)
library(stringr)
library(twitteR)
library(jsonlite)
line <- NULL
current.tweet <- NULL
tweet.table <- data.frame(stringsAsFactors = FALSE)
user.table <-NULL
#---1ST PART - SEARCHING TWITTER ------------------------------------------------------------------
setup_twitter_oauth("tHohRwSqdlB5OXokMm9gOCazW", "NWRp7Jcxv5qNe9scVo16lJ7atQaYH1Mo5FEmidGH2uJSwkbsWw","3909197654-FNrf9tlWwLwmCsV4zIneQcX1IEjmm3CBG9JoPrt", "mIf1nEWdkXaGOQFzCRGH1cPaGjVePOhCN3G0h2xJdA6zF")
search.terms <- c("whitney bank", "hancock bank", "harrison finance", "hancockbank", "whitneybank")
all.tweet.frame <- NULL
all.user.frame <- NULL
#---Seaching for tweets matching each term in the 'search.terms' variable -------------------------
for(h in 1:length(search.terms)){
tweet.frame <- NULL
search.key <- search.terms[h]
tweet.count <- 100
all.tweets <- searchTwitter(searchString = search.key, n=tweet.count,  resultType = "recent")
for(j in 1 : length(all.tweets)) {
current.tweet <- all.tweets[[j]]$toDataFrame()
tweet.frame <- rbind(tweet.frame,current.tweet)
}
tweet.text <- str_replace_all( tweet.frame$text,"[[:punct:]]", "")
tweet.text <- sapply(tweet.text, toupper)
tweet.include <- str_detect(tweet.text, toupper(search.key))
tweet.frame <- tweet.frame[tweet.include,]
unique.users <- unique(tweet.frame$screenName)
search <- rep(search.key, times=nrow(tweet.frame))
addeddate <- rep(Sys.Date(), times=nrow(tweet.frame))
tweet.frame <- cbind(tweet.frame,search,addeddate)
all.tweet.frame <- rbind(all.tweet.frame,tweet.frame)
}
#---Getting the unique users from the tweets in order to request their profile one time ------------
unique.users <- unique(all.tweet.frame$screenName)
unique.users.lookup <- lookupUsers(unique.users)
#---Requesting each user's profile from Twitter ---------------------------------------------------
for(j in 1:length(unique.users)){
current.user <- unique.users.lookup[[j]]
current.user.frame <- current.user$toDataFrame()
all.user.frame <- rbind(all.user.frame, current.user.frame)
}
tweets.file <- str_replace(paste(Sys.Date(), "-tweet.csv"), " ", "")
user.file <- str_replace(paste(Sys.Date(), "-user.csv"), " ", "")
#---Saving the tweets and users to their respective CSV file --------------------------------------
write.csv(all.tweet.frame, tweets.file)
write.csv(all.user.frame, user.file)
#---2ND PART - TWEET ANALYSIS ---------------------------------------------------------------------
#--Identifying all the CVS files in the Home directory and extrating the data from each -----------
get.files <- list.files()
tweets.location <- str_detect(get.files, "tweet.csv")
users.location <- str_detect(get.files, "user.csv")
tweet.files <- get.files[tweets.location]
user.files <- get.files[users.location]
tweet.table <-  read.csv(tweet.files[1])
for(j in 2:length(tweet.files)){tweet.table <-  rbind(tweet.table, read.csv(tweet.files[j]))}
tweet.dups <- duplicated(tweet.table[9])
tweet.dups <- tweet.dups==FALSE
tweet.table <- tweet.table[tweet.dups,]
user.table <-  read.csv(user.files[1])
for(j in 2:length(user.files)){user.table <-  rbind(user.table, read.csv(user.files[j]))}
user.dups <- duplicated(user.table[15])
user.dups <- user.dups==FALSE
user.table <- user.table[user.dups,]
tweet.table <- merge(x=tweet.table, y=user.table, by.x="screenName", by.y ="screenName")
#---Converting the 'search.terms' values to user-friendly values ----------------------------------
tweet.table$search <- as.character(tweet.table$search)
tweet.table$search[tweet.table$search=="hancockbank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="hancock bank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="whitneybank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="whitney bank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="harrisonfinance"]<-"Harrison Finance"
tweet.table$search[tweet.table$search=="harrison finance"]<-"Harrison Finance"
tweet.text <- tweet.table$text
tweet.ids <- tweet.table$id.x
tweet.text <- lapply(tweet.text, toupper)
tweet.text <- str_replace_all( tweet.text,"[[:punct:]]", "")
tweet.text <- str_replace_all(tweet.text,"\n", "")
#---Extracting each valid word from each tweet ----------------------------------------------------
word.table <-NULL
for(i in 1:length(tweet.text)){
newline <- strsplit(tweet.text[i], split=" ")
current.tweet <- as.data.frame(table(newline))
current.tweet <- cbind(current.tweet, rep(tweet.ids[i],nrow(current.tweet)))
word.table <- rbind(word.table, current.tweet)
}
colnames(word.table) <- c("line", "Freq", "ids")
#---Compiling and removing any undesired word from the list to streamline the analysis ------------
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
View(tweet.table)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_66')
library(rJava)
library(xlsx)
library(stringr)
library(twitteR)
library(jsonlite)
line <- NULL
current.tweet <- NULL
tweet.table <- data.frame(stringsAsFactors = FALSE)
user.table <-NULL
#---1ST PART - SEARCHING TWITTER ------------------------------------------------------------------
setup_twitter_oauth("tHohRwSqdlB5OXokMm9gOCazW", "NWRp7Jcxv5qNe9scVo16lJ7atQaYH1Mo5FEmidGH2uJSwkbsWw","3909197654-FNrf9tlWwLwmCsV4zIneQcX1IEjmm3CBG9JoPrt", "mIf1nEWdkXaGOQFzCRGH1cPaGjVePOhCN3G0h2xJdA6zF")
search.terms <- c("whitney bank", "hancock bank", "harrison finance", "hancockbank", "whitneybank")
all.tweet.frame <- NULL
all.user.frame <- NULL
#---Seaching for tweets matching each term in the 'search.terms' variable -------------------------
for(h in 1:length(search.terms)){
tweet.frame <- NULL
search.key <- search.terms[h]
tweet.count <- 100
all.tweets <- searchTwitter(searchString = search.key, n=tweet.count,  resultType = "recent")
for(j in 1 : length(all.tweets)) {
current.tweet <- all.tweets[[j]]$toDataFrame()
tweet.frame <- rbind(tweet.frame,current.tweet)
}
tweet.text <- str_replace_all( tweet.frame$text,"[[:punct:]]", "")
tweet.text <- sapply(tweet.text, toupper)
tweet.include <- str_detect(tweet.text, toupper(search.key))
tweet.frame <- tweet.frame[tweet.include,]
unique.users <- unique(tweet.frame$screenName)
search <- rep(search.key, times=nrow(tweet.frame))
addeddate <- rep(Sys.Date(), times=nrow(tweet.frame))
tweet.frame <- cbind(tweet.frame,search,addeddate)
all.tweet.frame <- rbind(all.tweet.frame,tweet.frame)
}
#---Getting the unique users from the tweets in order to request their profile one time ------------
unique.users <- unique(all.tweet.frame$screenName)
unique.users.lookup <- lookupUsers(unique.users)
#---Requesting each user's profile from Twitter ---------------------------------------------------
for(j in 1:length(unique.users)){
current.user <- unique.users.lookup[[j]]
current.user.frame <- current.user$toDataFrame()
all.user.frame <- rbind(all.user.frame, current.user.frame)
}
tweets.file <- str_replace(paste(Sys.Date(), "-tweet.csv"), " ", "")
user.file <- str_replace(paste(Sys.Date(), "-user.csv"), " ", "")
#---Saving the tweets and users to their respective CSV file --------------------------------------
write.csv(all.tweet.frame, tweets.file)
write.csv(all.user.frame, user.file)
#---2ND PART - TWEET ANALYSIS ---------------------------------------------------------------------
#--Identifying all the CVS files in the Home directory and extrating the data from each -----------
get.files <- list.files()
tweets.location <- str_detect(get.files, "tweet.csv")
users.location <- str_detect(get.files, "user.csv")
tweet.files <- get.files[tweets.location]
user.files <- get.files[users.location]
tweet.table <-  read.csv(tweet.files[1])
for(j in 2:length(tweet.files)){tweet.table <-  rbind(tweet.table, read.csv(tweet.files[j]))}
tweet.dups <- duplicated(tweet.table[9])
tweet.dups <- tweet.dups==FALSE
tweet.table <- tweet.table[tweet.dups,]
user.table <-  read.csv(user.files[1])
for(j in 2:length(user.files)){user.table <-  rbind(user.table, read.csv(user.files[j]))}
user.dups <- duplicated(user.table[15])
user.dups <- user.dups==FALSE
user.table <- user.table[user.dups,]
tweet.table <- merge(x=tweet.table, y=user.table, by.x="screenName", by.y ="screenName")
#---Converting the 'search.terms' values to user-friendly values ----------------------------------
tweet.table$search <- as.character(tweet.table$search)
tweet.table$search[tweet.table$search=="hancockbank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="hancock bank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="whitneybank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="whitney bank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="harrisonfinance"]<-"Harrison Finance"
tweet.table$search[tweet.table$search=="harrison finance"]<-"Harrison Finance"
tweet.text <- tweet.table$text
tweet.ids <- tweet.table$id.x
tweet.text <- lapply(tweet.text, toupper)
tweet.text <- str_replace_all( tweet.text,"[[:punct:]]", "")
tweet.text <- str_replace_all(tweet.text,"\n", "")
#---Extracting each valid word from each tweet ----------------------------------------------------
word.table <-NULL
for(i in 1:length(tweet.text)){
newline <- strsplit(tweet.text[i], split=" ")
current.tweet <- as.data.frame(table(newline))
current.tweet <- cbind(current.tweet, rep(tweet.ids[i],nrow(current.tweet)))
word.table <- rbind(word.table, current.tweet)
}
colnames(word.table) <- c("line", "Freq", "ids")
#---Compiling and removing any undesired word from the list to streamline the analysis ------------
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
View(tweet.table)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_66')
library(rJava)
library(xlsx)
library(stringr)
library(twitteR)
library(jsonlite)
line <- NULL
current.tweet <- NULL
tweet.table <- data.frame(stringsAsFactors = FALSE)
user.table <-NULL
#---1ST PART - SEARCHING TWITTER ------------------------------------------------------------------
setup_twitter_oauth("tHohRwSqdlB5OXokMm9gOCazW", "NWRp7Jcxv5qNe9scVo16lJ7atQaYH1Mo5FEmidGH2uJSwkbsWw","3909197654-FNrf9tlWwLwmCsV4zIneQcX1IEjmm3CBG9JoPrt", "mIf1nEWdkXaGOQFzCRGH1cPaGjVePOhCN3G0h2xJdA6zF")
search.terms <- c("whitney bank", "hancock bank", "harrison finance", "hancockbank", "whitneybank")
all.tweet.frame <- NULL
all.user.frame <- NULL
#---Seaching for tweets matching each term in the 'search.terms' variable -------------------------
for(h in 1:length(search.terms)){
tweet.frame <- NULL
search.key <- search.terms[h]
tweet.count <- 100
all.tweets <- searchTwitter(searchString = search.key, n=tweet.count,  resultType = "recent")
for(j in 1 : length(all.tweets)) {
current.tweet <- all.tweets[[j]]$toDataFrame()
tweet.frame <- rbind(tweet.frame,current.tweet)
}
tweet.text <- str_replace_all( tweet.frame$text,"[[:punct:]]", "")
tweet.text <- sapply(tweet.text, toupper)
tweet.include <- str_detect(tweet.text, toupper(search.key))
tweet.frame <- tweet.frame[tweet.include,]
unique.users <- unique(tweet.frame$screenName)
search <- rep(search.key, times=nrow(tweet.frame))
addeddate <- rep(Sys.Date(), times=nrow(tweet.frame))
tweet.frame <- cbind(tweet.frame,search,addeddate)
all.tweet.frame <- rbind(all.tweet.frame,tweet.frame)
}
#---Getting the unique users from the tweets in order to request their profile one time ------------
unique.users <- unique(all.tweet.frame$screenName)
unique.users.lookup <- lookupUsers(unique.users)
#---Requesting each user's profile from Twitter ---------------------------------------------------
for(j in 1:length(unique.users)){
current.user <- unique.users.lookup[[j]]
current.user.frame <- current.user$toDataFrame()
all.user.frame <- rbind(all.user.frame, current.user.frame)
}
tweets.file <- str_replace(paste(Sys.Date(), "-tweet.csv"), " ", "")
user.file <- str_replace(paste(Sys.Date(), "-user.csv"), " ", "")
#---Saving the tweets and users to their respective CSV file --------------------------------------
write.csv(all.tweet.frame, tweets.file)
write.csv(all.user.frame, user.file)
#---2ND PART - TWEET ANALYSIS ---------------------------------------------------------------------
#--Identifying all the CVS files in the Home directory and extrating the data from each -----------
get.files <- list.files()
tweets.location <- str_detect(get.files, "tweet.csv")
users.location <- str_detect(get.files, "user.csv")
tweet.files <- get.files[tweets.location]
user.files <- get.files[users.location]
tweet.table <-  read.csv(tweet.files[1])
for(j in 2:length(tweet.files)){tweet.table <-  rbind(tweet.table, read.csv(tweet.files[j]))}
tweet.dups <- duplicated(tweet.table[9])
tweet.dups <- tweet.dups==FALSE
tweet.table <- tweet.table[tweet.dups,]
user.table <-  read.csv(user.files[1])
for(j in 2:length(user.files)){user.table <-  rbind(user.table, read.csv(user.files[j]))}
user.dups <- duplicated(user.table[15])
user.dups <- user.dups==FALSE
user.table <- user.table[user.dups,]
tweet.table <- merge(x=tweet.table, y=user.table, by.x="screenName", by.y ="screenName")
#---Converting the 'search.terms' values to user-friendly values ----------------------------------
tweet.table$search <- as.character(tweet.table$search)
tweet.table$search[tweet.table$search=="hancockbank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="hancock bank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="whitneybank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="whitney bank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="harrisonfinance"]<-"Harrison Finance"
tweet.table$search[tweet.table$search=="harrison finance"]<-"Harrison Finance"
tweet.text <- tweet.table$text
tweet.ids <- tweet.table$id.x
tweet.text <- lapply(tweet.text, toupper)
tweet.text <- str_replace_all( tweet.text,"[[:punct:]]", "")
tweet.text <- str_replace_all(tweet.text,"\n", "")
#---Extracting each valid word from each tweet ----------------------------------------------------
word.table <-NULL
for(i in 1:length(tweet.text)){
newline <- strsplit(tweet.text[i], split=" ")
current.tweet <- as.data.frame(table(newline))
current.tweet <- cbind(current.tweet, rep(tweet.ids[i],nrow(current.tweet)))
word.table <- rbind(word.table, current.tweet)
}
colnames(word.table) <- c("line", "Freq", "ids")
#---Compiling and removing any undesired word from the list to streamline the analysis ------------
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
View(tweet.table)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_66')
library(rJava)
library(xlsx)
library(stringr)
library(twitteR)
library(jsonlite)
line <- NULL
current.tweet <- NULL
tweet.table <- data.frame(stringsAsFactors = FALSE)
user.table <-NULL
#---1ST PART - SEARCHING TWITTER ------------------------------------------------------------------
setup_twitter_oauth("tHohRwSqdlB5OXokMm9gOCazW", "NWRp7Jcxv5qNe9scVo16lJ7atQaYH1Mo5FEmidGH2uJSwkbsWw","3909197654-FNrf9tlWwLwmCsV4zIneQcX1IEjmm3CBG9JoPrt", "mIf1nEWdkXaGOQFzCRGH1cPaGjVePOhCN3G0h2xJdA6zF")
search.terms <- c("whitney bank", "hancock bank", "harrison finance", "hancockbank", "whitneybank")
all.tweet.frame <- NULL
all.user.frame <- NULL
#---Seaching for tweets matching each term in the 'search.terms' variable -------------------------
for(h in 1:length(search.terms)){
tweet.frame <- NULL
search.key <- search.terms[h]
tweet.count <- 100
all.tweets <- searchTwitter(searchString = search.key, n=tweet.count,  resultType = "recent")
for(j in 1 : length(all.tweets)) {
current.tweet <- all.tweets[[j]]$toDataFrame()
tweet.frame <- rbind(tweet.frame,current.tweet)
}
tweet.text <- str_replace_all( tweet.frame$text,"[[:punct:]]", "")
tweet.text <- sapply(tweet.text, toupper)
tweet.include <- str_detect(tweet.text, toupper(search.key))
tweet.frame <- tweet.frame[tweet.include,]
unique.users <- unique(tweet.frame$screenName)
search <- rep(search.key, times=nrow(tweet.frame))
addeddate <- rep(Sys.Date(), times=nrow(tweet.frame))
tweet.frame <- cbind(tweet.frame,search,addeddate)
all.tweet.frame <- rbind(all.tweet.frame,tweet.frame)
}
#---Getting the unique users from the tweets in order to request their profile one time ------------
unique.users <- unique(all.tweet.frame$screenName)
unique.users.lookup <- lookupUsers(unique.users)
#---Requesting each user's profile from Twitter ---------------------------------------------------
for(j in 1:length(unique.users)){
current.user <- unique.users.lookup[[j]]
current.user.frame <- current.user$toDataFrame()
all.user.frame <- rbind(all.user.frame, current.user.frame)
}
tweets.file <- str_replace(paste(Sys.Date(), "-tweet.csv"), " ", "")
user.file <- str_replace(paste(Sys.Date(), "-user.csv"), " ", "")
#---Saving the tweets and users to their respective CSV file --------------------------------------
write.csv(all.tweet.frame, tweets.file)
write.csv(all.user.frame, user.file)
#---2ND PART - TWEET ANALYSIS ---------------------------------------------------------------------
#--Identifying all the CVS files in the Home directory and extrating the data from each -----------
get.files <- list.files()
tweets.location <- str_detect(get.files, "tweet.csv")
users.location <- str_detect(get.files, "user.csv")
tweet.files <- get.files[tweets.location]
user.files <- get.files[users.location]
tweet.table <-  read.csv(tweet.files[1])
for(j in 2:length(tweet.files)){tweet.table <-  rbind(tweet.table, read.csv(tweet.files[j]))}
tweet.dups <- duplicated(tweet.table[9])
tweet.dups <- tweet.dups==FALSE
tweet.table <- tweet.table[tweet.dups,]
user.table <-  read.csv(user.files[1])
for(j in 2:length(user.files)){user.table <-  rbind(user.table, read.csv(user.files[j]))}
user.dups <- duplicated(user.table[15])
user.dups <- user.dups==FALSE
user.table <- user.table[user.dups,]
tweet.table <- merge(x=tweet.table, y=user.table, by.x="screenName", by.y ="screenName")
#---Converting the 'search.terms' values to user-friendly values ----------------------------------
tweet.table$search <- as.character(tweet.table$search)
tweet.table$search[tweet.table$search=="hancockbank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="hancock bank"]<-"Hancock Bank"
tweet.table$search[tweet.table$search=="whitneybank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="whitney bank"]<-"Whitney Bank"
tweet.table$search[tweet.table$search=="harrisonfinance"]<-"Harrison Finance"
tweet.table$search[tweet.table$search=="harrison finance"]<-"Harrison Finance"
tweet.text <- tweet.table$text
tweet.ids <- tweet.table$id.x
tweet.text <- lapply(tweet.text, toupper)
tweet.text <- str_replace_all( tweet.text,"[[:punct:]]", "")
tweet.text <- str_replace_all(tweet.text,"\n", "")
#---Extracting each valid word from each tweet ----------------------------------------------------
word.table <-NULL
for(i in 1:length(tweet.text)){
newline <- strsplit(tweet.text[i], split=" ")
current.tweet <- as.data.frame(table(newline))
current.tweet <- cbind(current.tweet, rep(tweet.ids[i],nrow(current.tweet)))
word.table <- rbind(word.table, current.tweet)
}
colnames(word.table) <- c("line", "Freq", "ids")
#---Compiling and removing any undesired word from the list to streamline the analysis ------------
exclude.words <- c("", "ANY","AND", "THE","IN","OR","ARE","ON","PUT","A","PUT","AT","RT","FOR","OF","THIS","I","IM","YOU", "ITS","THAT","OVER", "WAS", "WHITNEY","HANCOCK","HARRISON","FINANCE","BANK", "WHO","FROM","ELSE","NOT", "WASNT","DIDNT", "IS","BY","AS"," A","TO", "WHEN", "WHY","HOW","HANCOCKBANK","WHITNEYBANK","INTO", "AMP","OUR","WILL","HAVE", "WITH", "ALLPHOTOS", "OUR")
for (i in 1:length(exclude.words)){word.table <- word.table[(word.table$line==exclude.words[i])==FALSE,]}
word.table <- word.table[nchar(as.vector(word.table$line))<40,]
word.table <- word.table[substr(word.table$line,1,5) != "HTTPS",]
word.table <- aggregate(Freq~line+ids, data=word.table, FUN=sum)
colnames(word.table) <- c("tweet_word", "tweet_id", "word_count")
#---Removing undesired columns and renaming the remaining columns with user-friendly names --------
tweet.table <- tweet.table[,c(1,3,7,10,13,14,18,19,21,23,24,25,27,28,31,32,33)]
colnames(tweet.table) <- c("user_screenName","tweet_text","tweet_created","tweet_id","tweet_retweetCount","tweet_isRetweet","keyword_search","tweet_founddate","user_description","user_followersCount","user_favoritesCount","user_friendsCount","user_name","user_dateadded","user_location","user_lang","user_id")
#---Merging the 'word' and 'tweet' table into one and saving the results to an Excel file ---------
full.table <- merge(x=tweet.table, y=word.table, by.x = "tweet_id", by.y = "tweet_id")
#write.xlsx(tweet.table, file="tweets.xlsx", sheetName="tweets")
#write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=TRUE)
write.xlsx(full.table, file="tweets.xlsx", sheetName="word", append=)
print(Sys.time())
View(tweet.table)
setwd("C:/Users/Edgar/Desktop/Summary of Deposits")
